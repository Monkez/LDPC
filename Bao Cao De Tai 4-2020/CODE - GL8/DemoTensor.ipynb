{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T08:19:03.771871Z",
     "start_time": "2019-07-18T08:19:03.764890Z"
    },
    "code_folding": [
     4,
     9,
     38
    ]
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class GF():\n",
    "    def __init__(self, f):\n",
    "        self.len_of_symbol = len(f)-1\n",
    "        self.q = 2**self.len_of_symbol\n",
    "        self.table, self.tf_table = np.array(self.make_code_table(f))\n",
    "        \n",
    "    def make_code_table(self, f):\n",
    "        m = len(f)-1\n",
    "        N = 2**m\n",
    "        table = [\"0\"*m]\n",
    "        tf_table = []\n",
    "        x =\"\"\n",
    "        for i in range(1,m+1):\n",
    "            if f[i]==0:\n",
    "                x+=\"0\"\n",
    "            else:\n",
    "                x+=\"1\"\n",
    "        for i in range(0, m):\n",
    "            c=\"0\"*m\n",
    "            c = c[0:i] + \"1\"+c[i+1:]\n",
    "            c = c[::-1]\n",
    "            table.append(c)\n",
    "        for i in range(m+1, N):\n",
    "            a = table[-1]\n",
    "            a1 = a[1:]+\"0\"\n",
    "            b = \"0\"*m\n",
    "            if a[0]==\"1\":\n",
    "                b = x\n",
    "            c = self.add_string_code(a1, b)\n",
    "            table.append(c)\n",
    "            vec = [False if ci == 0 else True for ci in c ]\n",
    "            tf_vec = tf.constant(vec, dtype = tf.bool) \n",
    "            tf_table.append(tf_vec)\n",
    "        return table, tf_table\n",
    "\n",
    "    def add_string_code(self, a, b):\n",
    "        c =\"\"\n",
    "        for i in range(self.len_of_symbol):\n",
    "            if a[i]!= b[i] :\n",
    "                c+=\"1\"\n",
    "            else:\n",
    "                c+=\"0\"\n",
    "        return c\n",
    "    \n",
    "    def tensor(self, a):\n",
    "        if len(a.shape) ==1:\n",
    "            return self.tf_table(a)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T08:20:07.851707Z",
     "start_time": "2019-07-18T08:20:07.797851Z"
    }
   },
   "outputs": [],
   "source": [
    "gl4 = GF([1, 1, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T02:57:45.213658Z",
     "start_time": "2019-07-21T02:57:37.879890Z"
    },
    "code_folding": [
     15,
     41,
     52,
     83
    ]
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import check_ops\n",
    "from tensorflow.python.ops import gen_nn_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import random_ops\n",
    "\n",
    "def _flatten_outer_dims(logits):\n",
    "  \"\"\"Flattens logits' outer dimensions and keep its last dimension.\"\"\"\n",
    "  rank = array_ops.rank(logits)\n",
    "  last_dim_size = array_ops.slice(\n",
    "      array_ops.shape(logits), [math_ops.subtract(rank, 1)], [1])\n",
    "  output = array_ops.reshape(logits, array_ops.concat([[-1], last_dim_size], 0))\n",
    "\n",
    "  # Set output shape if known.\n",
    "  if not context.executing_eagerly():\n",
    "    shape = logits.get_shape()\n",
    "    if shape is not None and shape.dims is not None:\n",
    "      shape = shape.as_list()\n",
    "      product = 1\n",
    "      product_valid = True\n",
    "      for d in shape[:-1]:\n",
    "        if d is None:\n",
    "          product_valid = False\n",
    "          break\n",
    "        else:\n",
    "          product *= d\n",
    "      if product_valid:\n",
    "        output_shape = [product, shape[-1]]\n",
    "        output.set_shape(output_shape)\n",
    "\n",
    "  return output\n",
    "\n",
    "def _swap_axis(logits, dim_index, last_index, name=None):\n",
    "        \"\"\"Swaps logits's dim_index and last_index.\"\"\"\n",
    "        return array_ops.transpose(\n",
    "            logits,\n",
    "            array_ops.concat([\n",
    "                math_ops.range(dim_index), [last_index],\n",
    "                math_ops.range(dim_index + 1, last_index), [dim_index]\n",
    "            ], 0),\n",
    "            name=name)\n",
    "    \n",
    "\n",
    "def reshape(logits, compute_op, dim=-1, name=None):\n",
    "    logits = ops.convert_to_tensor(logits)\n",
    "    # We need its original shape for shape inference.\n",
    "    shape = logits.get_shape()\n",
    "    is_last_dim = (dim is -1) or (dim == shape.ndims - 1)\n",
    "\n",
    "    if shape.ndims is 2 and is_last_dim:\n",
    "        return compute_op(logits, name=name)\n",
    "\n",
    "    # If dim is the last dimension, simply reshape the logits to a matrix and\n",
    "    # apply the internal softmax.\n",
    "    if is_last_dim:\n",
    "        input_shape = array_ops.shape(logits)\n",
    "        logits = _flatten_outer_dims(logits)\n",
    "        output = compute_op(logits)\n",
    "        output = array_ops.reshape(output, input_shape, name=name)\n",
    "        return output\n",
    "\n",
    "    # If dim is not the last dimension, we have to do a reshape and transpose so\n",
    "    # that we can still perform softmax on its last dimension.\n",
    "\n",
    "    # Swap logits' dimension of dim and its last dimension.\n",
    "    input_rank = array_ops.rank(logits)\n",
    "    dim_axis = dim % shape.ndims\n",
    "    logits = _swap_axis(logits, dim_axis, math_ops.subtract(input_rank, 1))\n",
    "    shape_after_swap = array_ops.shape(logits)\n",
    "\n",
    "    # Reshape logits into a matrix.\n",
    "    logits = _flatten_outer_dims(logits)\n",
    "    return logits,  shape_after_swap, dim_axis, compute_op, input_rank, shape\n",
    "\n",
    "def unreshape(logits, shape_after_swap, dim_axis, compute_op, input_rank, shape):\n",
    "    # Do the actual softmax on its last dimension.\n",
    "    output = compute_op(logits)\n",
    "\n",
    "    # Transform back the output tensor.\n",
    "    output = array_ops.reshape(output, shape_after_swap)\n",
    "    output = _swap_axis(\n",
    "      output, dim_axis, math_ops.subtract(input_rank, 1))\n",
    "\n",
    "    # Make shape inference work since reshape and transpose may erase its static\n",
    "    # shape.\n",
    "    output.set_shape(shape)\n",
    "\n",
    "    return output\n",
    "\n",
    "config=tf.ConfigProto(log_device_placement=True)\n",
    "## GLOBAL CONFIG\n",
    "TENSOR_DTYPE = tf.float16\n",
    "NUMPY_DTYPE = np.float16\n",
    "MaxIter = 15\n",
    "batch_size = 3\n",
    "A = 2\n",
    "N = 6\n",
    "K= 3\n",
    "M = N - K\n",
    "f = np.random.randint(1, 10, ( batch_size, A, N))\n",
    "print(f)\n",
    "print(\"#\"*30)\n",
    "## GRAPH\n",
    "s1 = time.time()\n",
    "P = tf.placeholder(TENSOR_DTYPE, [None, A, N], name= \"Input\")\n",
    "L,  shape_after_swap, dim_axis, compute_op, input_rank, shape = reshape(P, gen_nn_ops.softmax, 1)\n",
    "L = tf.transpose(L)\n",
    "L = L/tf.reduce_sum(L, axis= 0)\n",
    "L = tf.transpose(L)\n",
    "L = unreshape(L, shape_after_swap, dim_axis, compute_op, input_rank, shape)\n",
    "Q = tf.expand_dims(L, 1)   \n",
    "Q = tf.tile(Q, [  1, M,  1, 1])\n",
    "Q = tf.reshape(Q, [batch_size,   M, A,  N])\n",
    "Z = tf.arg_min(Q, 2)\n",
    "model = tf.global_variables_initializer()\n",
    "sess =  tf.Session(config=config)\n",
    "sess.run(model)\n",
    "\n",
    "_L = sess.run(Q, feed_dict={P:f})\n",
    "print(_L.shape)\n",
    "print(_L)\n",
    "s2 = time.time()\n",
    "print(s2- s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T08:35:02.238229Z",
     "start_time": "2019-07-21T08:35:02.234239Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "config=tf.ConfigProto(log_device_placement=True)\n",
    "A = 2\n",
    "N = 6\n",
    "K= 3\n",
    "M = N-K\n",
    "batch_size = 200\n",
    "rand = np.random.randint(1, 10, ( batch_size, A, N))\n",
    "#print(rand)\n",
    "_GF2 = np.array([\n",
    "    [1, 0],\n",
    "    [0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T08:35:04.332008Z",
     "start_time": "2019-07-21T08:35:04.027082Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2388594150543213\n",
      "(200, 3, 2, 6)\n",
      "[[[[7. 3. 4. 2. 3. 4.]\n",
      "   [2. 9. 2. 6. 6. 8.]]\n",
      "\n",
      "  [[7. 3. 4. 2. 3. 4.]\n",
      "   [2. 9. 2. 6. 6. 8.]]\n",
      "\n",
      "  [[7. 3. 4. 2. 3. 4.]\n",
      "   [2. 9. 2. 6. 6. 8.]]]\n",
      "\n",
      "\n",
      " [[[4. 2. 5. 2. 5. 6.]\n",
      "   [3. 3. 9. 8. 7. 2.]]\n",
      "\n",
      "  [[4. 2. 5. 2. 5. 6.]\n",
      "   [3. 3. 9. 8. 7. 2.]]\n",
      "\n",
      "  [[4. 2. 5. 2. 5. 6.]\n",
      "   [3. 3. 9. 8. 7. 2.]]]\n",
      "\n",
      "\n",
      " [[[3. 1. 5. 9. 9. 5.]\n",
      "   [7. 7. 7. 5. 6. 4.]]\n",
      "\n",
      "  [[3. 1. 5. 9. 9. 5.]\n",
      "   [7. 7. 7. 5. 6. 4.]]\n",
      "\n",
      "  [[3. 1. 5. 9. 9. 5.]\n",
      "   [7. 7. 7. 5. 6. 4.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[4. 4. 8. 6. 6. 6.]\n",
      "   [6. 4. 1. 4. 2. 9.]]\n",
      "\n",
      "  [[4. 4. 8. 6. 6. 6.]\n",
      "   [6. 4. 1. 4. 2. 9.]]\n",
      "\n",
      "  [[4. 4. 8. 6. 6. 6.]\n",
      "   [6. 4. 1. 4. 2. 9.]]]\n",
      "\n",
      "\n",
      " [[[1. 5. 2. 1. 5. 6.]\n",
      "   [1. 6. 5. 1. 6. 4.]]\n",
      "\n",
      "  [[1. 5. 2. 1. 5. 6.]\n",
      "   [1. 6. 5. 1. 6. 4.]]\n",
      "\n",
      "  [[1. 5. 2. 1. 5. 6.]\n",
      "   [1. 6. 5. 1. 6. 4.]]]\n",
      "\n",
      "\n",
      " [[[3. 6. 6. 7. 3. 3.]\n",
      "   [8. 9. 9. 4. 6. 5.]]\n",
      "\n",
      "  [[3. 6. 6. 7. 3. 3.]\n",
      "   [8. 9. 9. 4. 6. 5.]]\n",
      "\n",
      "  [[3. 6. 6. 7. 3. 3.]\n",
      "   [8. 9. 9. 4. 6. 5.]]]]\n",
      "(200, 3, 6)\n",
      "[[[1 0 1 0 0 0]\n",
      "  [1 0 1 0 0 0]\n",
      "  [1 0 1 0 0 0]]\n",
      "\n",
      " [[1 0 0 0 0 1]\n",
      "  [1 0 0 0 0 1]\n",
      "  [1 0 0 0 0 1]]\n",
      "\n",
      " [[0 0 0 1 1 1]\n",
      "  [0 0 0 1 1 1]\n",
      "  [0 0 0 1 1 1]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 1 1 1 0]\n",
      "  [0 0 1 1 1 0]\n",
      "  [0 0 1 1 1 0]]\n",
      "\n",
      " [[0 0 0 0 0 1]\n",
      "  [0 0 0 0 0 1]\n",
      "  [0 0 0 0 0 1]]\n",
      "\n",
      " [[0 0 0 1 0 0]\n",
      "  [0 0 0 1 0 0]\n",
      "  [0 0 0 1 0 0]]]\n",
      "(200, 3)\n",
      "[[2 2 2]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [4 4 4]\n",
      " [3 3 3]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [2 2 2]\n",
      " [0 0 0]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [1 1 1]\n",
      " [2 2 2]\n",
      " [4 4 4]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [4 4 4]\n",
      " [1 1 1]\n",
      " [3 3 3]\n",
      " [3 3 3]\n",
      " [4 4 4]\n",
      " [3 3 3]\n",
      " [4 4 4]\n",
      " [0 0 0]\n",
      " [2 2 2]\n",
      " [4 4 4]\n",
      " [3 3 3]\n",
      " [3 3 3]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [1 1 1]\n",
      " [3 3 3]\n",
      " [1 1 1]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [1 1 1]\n",
      " [3 3 3]\n",
      " [3 3 3]\n",
      " [1 1 1]\n",
      " [2 2 2]\n",
      " [0 0 0]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [3 3 3]\n",
      " [6 6 6]\n",
      " [3 3 3]\n",
      " [5 5 5]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [1 1 1]\n",
      " [5 5 5]\n",
      " [1 1 1]\n",
      " [4 4 4]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [3 3 3]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [0 0 0]\n",
      " [1 1 1]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [0 0 0]\n",
      " [4 4 4]\n",
      " [1 1 1]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [3 3 3]\n",
      " [4 4 4]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [1 1 1]\n",
      " [3 3 3]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [0 0 0]\n",
      " [3 3 3]\n",
      " [5 5 5]\n",
      " [4 4 4]\n",
      " [2 2 2]\n",
      " [4 4 4]\n",
      " [5 5 5]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [4 4 4]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [3 3 3]\n",
      " [1 1 1]\n",
      " [3 3 3]\n",
      " [3 3 3]\n",
      " [4 4 4]\n",
      " [2 2 2]\n",
      " [5 5 5]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [1 1 1]\n",
      " [3 3 3]\n",
      " [4 4 4]\n",
      " [4 4 4]\n",
      " [3 3 3]\n",
      " [5 5 5]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [4 4 4]\n",
      " [2 2 2]\n",
      " [5 5 5]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [1 1 1]\n",
      " [4 4 4]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [0 0 0]\n",
      " [1 1 1]\n",
      " [4 4 4]\n",
      " [3 3 3]\n",
      " [3 3 3]\n",
      " [1 1 1]\n",
      " [3 3 3]\n",
      " [4 4 4]\n",
      " [4 4 4]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [4 4 4]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [4 4 4]\n",
      " [2 2 2]\n",
      " [0 0 0]\n",
      " [3 3 3]\n",
      " [3 3 3]\n",
      " [4 4 4]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 0 0]\n",
      " [1 1 1]\n",
      " [4 4 4]\n",
      " [3 3 3]\n",
      " [1 1 1]\n",
      " [3 3 3]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [3 3 3]\n",
      " [2 2 2]\n",
      " [1 1 1]\n",
      " [6 6 6]\n",
      " [1 1 1]\n",
      " [3 3 3]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "GF2 = tf.constant(_GF2)\n",
    "f = tf.placeholder(tf.float16, [None, A, N])\n",
    "L =  tf.expand_dims(f, 1)   \n",
    "Q =  tf.tile(L, [ 1, M, 1, 1]) \n",
    "Z = tf.arg_min(Q, dimension=2)\n",
    "## Step 1\n",
    "transposedQ = tf.transpose(Q, perm=[2, 0, 1, 3])\n",
    "reshapedQ = tf.transpose(tf.reshape(transposedQ, [A, batch_size*M*N]))\n",
    "Z2= tf.argmax(reshapedQ, axis= 1)\n",
    "delQ_index_colum = tf.gather(GF2, Z2)\n",
    "delQ_index_colum = tf.reshape(delQ_index_colum, [1, batch_size*A*M*N])\n",
    "delQ_index_row = tf.transpose(tf.expand_dims(tf.range(batch_size*N*M), 0))\n",
    "delQ_index_row = tf.tile(delQ_index_row, [1, A])\n",
    "delQ_index_row = tf.reshape(delQ_index_row, [1, batch_size*A*N*M])\n",
    "delQ_index =  tf.concat([delQ_index_row, delQ_index_colum], axis=0)\n",
    "delQ_index = tf.transpose(delQ_index)\n",
    "delQ_index = tf.reshape(delQ_index, [batch_size*M*N,  A, 2])\n",
    "deltaQ = tf.gather_nd(reshapedQ, delQ_index)\n",
    "deltaQ = tf.transpose(deltaQ)\n",
    "deltaQ =tf.reshape(deltaQ, [A, batch_size, M, N])\n",
    "deltaQ = tf.transpose(deltaQ, perm=[1, 2, 0, 3])\n",
    "\n",
    "## Step 2\n",
    "Beta = tf.reduce_sum(Z, axis= 2)\n",
    "\n",
    "## Step 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sess =  tf.Session(config=config)\n",
    "s1 = time.time()\n",
    "X = sess.run([Q, Z, Beta], feed_dict={f:rand})\n",
    "s2 = time.time()\n",
    "print (s2- s1)\n",
    "for i in range(len(X)):\n",
    "    print(X[i].shape)\n",
    "    print(X[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T04:40:19.560314Z",
     "start_time": "2019-07-21T04:40:19.494502Z"
    }
   },
   "outputs": [],
   "source": [
    "grap2 = grap.copy()\n",
    "N = 120\n",
    "for i in range(N):\n",
    "    grap2[i] = (i+1)*grap[2]/grap[i]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(grap2[0:N])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T06:25:01.303600Z",
     "start_time": "2019-07-19T06:25:01.241766Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "a = tf.constant(np.random.randint(-10, 10, (1000, 100)), tf.float32)\n",
    "result = tf.reduce_max(a, axis=0)\n",
    "sess =  tf.Session()\n",
    "writer = tf.summary.FileWriter('./graphs', tf.get_default_graph())\n",
    "s1 = time.time()\n",
    "_L = sess.run(result)\n",
    "s2 = time.time()\n",
    "print(_L.shape)\n",
    "print(_L)\n",
    "print(s2- s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T10:53:53.596328Z",
     "start_time": "2019-07-19T10:53:53.592334Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.randint(0,3, (2))\n",
    "print(x)\n",
    "index = [1, 0]\n",
    "print(x[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
